{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b445c3-3371-4869-b7af-4da0ce96e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def query_model(prompt, model=\"llama3.2:3b\", url=\"http://localhost:11434/api/chat\", role=\"user\"):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"top_p\": 1,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text1\": {\"type\": \"string\", \"maxLength\": 150},\n",
    "                    \"text2\": {\"type\": \"string\", \"maxLength\": 150},\n",
    "                    \"paraphrase\": {\"type\": \"boolean\"}\n",
    "                },\n",
    "                \"required\": [\"text1\", \"text2\", \"paraphrase\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"messages\": [\n",
    "            {\"role\": role, \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea14882-1699-423c-be22-b872eafe17b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "  \t\t     \t\t\t  \t\t  \t\t\n"
     ]
    }
   ],
   "source": [
    "query = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\n",
    "query = \"what is the color of the sky at different times of day, respond in json\"\n",
    "result = query_model(query, role=\"assistant\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e93c7a-f612-4428-bff8-44dceb582e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{ \"text1\": \"The company will hold an annual meeting.\", \"text2\": \"The company will have its yearly gathering.\", \"paraphrase\": true }, { \"text1\": \"I love to play football.\", \"text2\": \"I am passionate about soccer.\", \"paraphrase\": false }, { \"text1\": \"He is going to the store.\", \"text2\": \"He is off to purchase something.\", \"paraphrase\": true }, { \"text1\": \"The new policy has been implemented.\", \"text2\": \"A new law has taken effect today.\", \"paraphrase\": true }, { \"text1\": \"I am a software engineer.\", \"text2\": \"I design and develop software.\", \"paraphrase\": true }, { \"text1\": \"The sun is shining brightly.\", \"text2\": \"It's a beautiful day outside.\", \"paraphrase\": true }, { \"text1\": \"She has been studying English for three years.\", \"text2\": \"She has been learning English for years.\", \"paraphrase\": true }, { \"text1\": \"The project is expected to be completed by the end of next month.\", \"text2\": \"We anticipate finishing the project by the last day of next month.\", \"paraphrase\": true }, { \"text1\": \"I am not a fan of coffee.\", \"text2\": \"I do not enjoy drinking coffee.\", \"paraphrase\": true }, { \"text1\": \"The hotel room is located on the ground floor.\", \"text2\": \"Our room is situated on the first level.\", \"paraphrase\": true }]\n"
     ]
    }
   ],
   "source": [
    "template = \"Your task is to generate dataset for an ai for paraphrase detection system that when given two text1 and text2 tells similarity between the two in english only. so write 10 such examples in json format like {'text1': 'today is monday', 'text2':'it is monday today', 'paraphrase': true} \"\n",
    "result = query_model(template)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00ca212-7bff-4e68-b39d-8994aa97d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to paraphrase_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_and_save_paraphrase_data(file_path=\"paraphrase_data.json\"):\n",
    "    template = \"\"\"Your task is to generate dataset for an AI paraphrase detection system. \n",
    "    Create 10 examples in JSON format like {'text1': '...', 'text2': '...', 'paraphrase': true/false}.\n",
    "    Include both positive and negative examples (some non-paraphrases).\"\"\"\n",
    "    \n",
    "    # Get raw response from LLM\n",
    "    result = query_model(template)\n",
    "    \n",
    "    try:\n",
    "        # Extract JSON array from markdown response\n",
    "        json_start = result.find('[')\n",
    "        json_end = result.rfind(']') + 1\n",
    "        json_data = json.loads(result[json_start:json_end])\n",
    "        \n",
    "        # Create structured format with metadata\n",
    "        structured_data = {\n",
    "            \"metadata\": {\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"task\": \"paraphrase_detection\",\n",
    "                \"version\": \"1.0\"\n",
    "            },\n",
    "            \"dataset\": json_data\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(structured_data, f, indent=4)\n",
    "            \n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "        \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        print(\"Raw response:\")\n",
    "        print(result)\n",
    "\n",
    "# Example usage\n",
    "generate_and_save_paraphrase_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1020f0d-ab43-4c8f-b549-0fe15468d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class ParaphraseDataGenerator:\n",
    "    def __init__(self, target_samples=100000, batch_size=200):\n",
    "        self.target_samples = target_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.output_file = \"paraphrase_data.jsonl\"\n",
    "        self.checkpoint_file = \"checkpoint.txt\"\n",
    "        self.current_count = 0\n",
    "        self._load_checkpoint()\n",
    "        \n",
    "    def _generate_batch(self):\n",
    "        prompt = f\"\"\"Generate {self.batch_size} diverse paraphrase pairs in english only with:\n",
    "        - Strict JSON format ONLY, no extra text\n",
    "        - All boolean values as true/false (lowercase)\n",
    "        - Properly escaped quotes\n",
    "        - No trailing commas\n",
    "        Return ONLY this format:\n",
    "        [{{\"text1\": \"...\", \"text2\": \"...\", \"paraphrase\": bool}}]\"\"\"\n",
    "        \n",
    "        retries = 5  # Increased retries\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = query_model(prompt)\n",
    "                # print(\"Raw response:\", response)  # Debug logging\n",
    "                \n",
    "                # Enhanced JSON extraction\n",
    "                cleaned_response = self._clean_response(response)\n",
    "                return self._safe_json_parse(cleaned_response)\n",
    "                \n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"Attempt {attempt+1}/{retries} failed: {str(e)}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        raise Exception(f\"Failed after {retries} attempts. Last response: {response[:200]}\")\n",
    "\n",
    "    def _clean_response(self, response):\n",
    "        # Remove markdown code blocks\n",
    "        response = re.sub(r'```json\\n?|```', '', response)\n",
    "        \n",
    "        # Remove non-JSON characters before first [ and after last ]\n",
    "        json_start = response.find('[')\n",
    "        json_end = response.rfind(']') + 1\n",
    "        if json_start == -1 or json_end == 0:\n",
    "            raise ValueError(\"No JSON array found in response\")\n",
    "            \n",
    "        return response[json_start:json_end]\n",
    "\n",
    "    def _safe_json_parse(self, json_str):\n",
    "        # First try parsing as normal\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Standard parse failed, trying recovery: {e}\")\n",
    "\n",
    "        # If that fails, try finding individual JSON objects\n",
    "        objects = []\n",
    "        decoder = json.JSONDecoder()\n",
    "        offset = 0\n",
    "        \n",
    "        # Remove whitespace and problematic characters\n",
    "        json_str = json_str.strip().replace('\\n', ' ').replace('\\t', ' ')\n",
    "        \n",
    "        while offset < len(json_str):\n",
    "            try:\n",
    "                # Skip commas between objects\n",
    "                while offset < len(json_str) and json_str[offset] in (' ', ',', '\\n', '\\r', '\\t'):\n",
    "                    offset += 1\n",
    "                \n",
    "                if offset >= len(json_str):\n",
    "                    break\n",
    "\n",
    "                obj, offset = decoder.raw_decode(json_str, idx=offset)\n",
    "                if isinstance(obj, dict):\n",
    "                    objects.append(obj)\n",
    "                elif isinstance(obj, list):\n",
    "                    objects.extend(obj)\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON at position {offset}: {e}\")\n",
    "                # Try to find next valid object\n",
    "                next_brace = json_str.find('{', offset + 1)\n",
    "                if next_brace == -1:\n",
    "                    break\n",
    "                offset = next_brace\n",
    "                \n",
    "        # Validate objects\n",
    "        valid_objects = []\n",
    "        for obj in objects:\n",
    "            if all(key in obj for key in ['text1', 'text2', 'paraphrase']):\n",
    "                # Fix common boolean issues\n",
    "                if isinstance(obj['paraphrase'], str):\n",
    "                    obj['paraphrase'] = obj['paraphrase'].lower() == 'true'\n",
    "                valid_objects.append(obj)\n",
    "        \n",
    "        if not valid_objects:\n",
    "            raise ValueError(\"No valid objects found after parsing\")\n",
    "            \n",
    "        print(f\"Recovered {len(valid_objects)} valid objects from batch\")\n",
    "        return valid_objects\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                self.current_count = int(f.read().strip())\n",
    "            print(f\"Resuming from checkpoint: {self.current_count}/{self.target_samples}\")\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            f.write(str(self.current_count))\n",
    "\n",
    "    def _save_batch(self, batch):\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            for item in batch:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    def generate(self):\n",
    "        pbar = tqdm(total=self.target_samples, initial=self.current_count)\n",
    "        \n",
    "        while self.current_count < self.target_samples:\n",
    "            batch = self._generate_batch()\n",
    "            valid_batch = [item for item in batch if all(k in item for k in ['text1', 'text2', 'paraphrase'])]\n",
    "            \n",
    "            self._save_batch(valid_batch)\n",
    "            self.current_count += len(valid_batch)\n",
    "            \n",
    "            # Update progress and checkpoint\n",
    "            pbar.update(len(valid_batch))\n",
    "            self._save_checkpoint()\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1.5)  # Adjust based on API limits\n",
    "            \n",
    "            # Memory management\n",
    "            if self.current_count % 10000 == 0:\n",
    "                print(f\"Intermediate checkpoint: {self.current_count} samples\")\n",
    "                \n",
    "        pbar.close()\n",
    "        print(f\"Completed generating {self.current_count} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c157f9-83b9-45e2-89a2-0c5406d45469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: 1001/10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c8ca68ebd047e6a945930b4d66f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 10%|#         | 1001/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 10001 samples\n"
     ]
    }
   ],
   "source": [
    "# Test with 100 samples first\n",
    "test_generator = ParaphraseDataGenerator(target_samples=10000, batch_size=10)\n",
    "test_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecaa26c1-7fa1-499d-a39a-8ec1814c2bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text1': 'The company has decided to increase salaries.', 'text2': 'The company will boost its pay.', 'paraphrase': True}\n",
      "{'text1': 'This is a very long text that needs to be rewritten.', 'text2': 'We need to shorten this lengthy sentence.', 'paraphrase': False}\n",
      "{'text1': 'The new employee was struggling with the workload.', 'text2': 'The new guy was overwhelmed by his tasks.', 'paraphrase': True}\n",
      "{'text1': 'It is essential that you complete your homework on time.', 'text2': 'Make sure to finish your assignments ahead of schedule.', 'paraphrase': True}\n",
      "{'text1': 'The city has implemented a new recycling program.', 'text2': 'We are introducing an eco-friendly waste collection system.', 'paraphrase': True}\n"
     ]
    }
   ],
   "source": [
    "def inspect_data(num_samples=5):\n",
    "    with open('paraphrase_data.jsonl', 'r') as f:\n",
    "        for _ in range(num_samples):\n",
    "            print(json.loads(f.readline()))\n",
    "inspect_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f8ec03-a1da-4a97-93c5-1758cfda1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c4cedd-ea10-4042-8ef4-70e4556f4c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4559]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [\"This is an apple\", \"This is a cat\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "sim = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "print(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8d4bef6-b899-4665-bb2c-8d2342fe8462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparaphrase_data.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m   1029\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     )\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1409\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_json(\"paraphrase_data.jsonl\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbbde284-4af5-4f95-91c0-58325df20a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim//num_heads\n",
    "        self.wq = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wk = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wv = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d = x.shape\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(b, s, self.num_heads, self.head_dim)\n",
    "        k = k.view(b, s, self.num_heads, self.head_dim)\n",
    "        v = v.view(b, s, self.num_heads, self.head_dim)\n",
    "\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        atten_score = torch.softmax((q@k.transpose(2,3))/(self.head_dim**0.5), dim=-1)\n",
    "        o = atten_score@v\n",
    "        o = o.transpose(1,2).contiguous().view(b, s, d)\n",
    "        return self.wo(o)\n",
    "        \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.layernorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4*hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.layernorm2(x)\n",
    "        return x\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,hidden_dim)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(self.hidden_dim, self.num_heads) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210c1b57-0723-4d18-91e5-242f49b928b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 80, (16, 16))\n",
    "model = Model(100, 64, 4, 4)\n",
    "x = model(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9afe7e-c47b-4223-aa81-08b9a55ea22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyper params\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "vocab_size = 1000\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "model = Model(vocab_size, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropy()\n",
    "dataloader = DataLoader()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    n = len(dataloader)\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        l = loss(logits, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=l.item()\n",
    "        \n",
    "    print(f\"Epoch : {epoch} || Loss : {l.item()}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
